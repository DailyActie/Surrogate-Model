"""Adaptive Neural Network Genetic Algorithm (ANGA)

Components:
* Genetic Algorithm
* Artificial Neural Networks
* Caching

Implementation:
* Fitness Sampling
    * Sampling Rate: Sampling rate determines how many individuals should be sampled from a population in each generation
    * Sampling Selection Strategy: Sampling selection strategy determines which individuals should be sampled from a population, given the current sampling rate.
    random sampling, best sampling, tournament sampling, combined tournament+best sampling
* ANN Training and Retraining
    * InitialTrainingGenerations: Alloftheindividualsin the first several generations of ANGA must be evaluated by the simulation models to generate the ANN training set.
    * Retraining Set Management: As more and more sampled solutions are generated from simulation model evaluations, the retraining set must be managed.
    growing set approach, fixed set approach
    * Retraining Method: When the ANNs need to be retrained, the training algorithm can either load the previ- ously trained weights and continue the training episodes on the new training set, or it can re-initialize the ANN weights to random values and completely retrain the ANNs.
    * Retraining Frequency: Retraining frequency deter- mines when the ANNs should be updated during an ANGA run. Retraining frequency should decrease in later gener- ations as the search progresses into relatively smoother local regions.
"""


# from surrogate.base import SurrogateModel
# class ANGA(object):
# class ANGA(SurrogateModel):
#     def __init__(self, x, y):
#         super(ANGA, self).__init__()
#
#         self.x = x
#         self.y = y
#
#     def predict_proba(self, x):
#         super(ANGA, self).predict_proba(x)
#         pass
#
#
# if __name__ == "__main__":
#     from sklearn import datasets
#
#     iris = datasets.load_iris()
#     X, y = iris.data, iris.target
#
#     # def branin(x):
#     #     y = (x[1] - (5.1 / (4. * pi ** 2.)) * x[0] ** 2. + 5. * x[0] / pi - 6.) ** 2. + 10. * (
#     #     1. - 1. / (8. * pi)) * cos(
#     #         x[0]) + 10.
#     #     return y
#     # def branin_1d(x):
#     #     return branin(array([x[0], 2.275]))
#     # X = array([[0.0], [2.0], [3.0], [4.0], [6.0]])
#     # y = array([[branin_1d(case)] for case in X])
#
#     anga = ANGA(X, y)
#     anga.fit(X, y)
#     anga.predict_proba(X)


import warnings

warnings.filterwarnings(action="ignore", category=Warning)
# warnings.filterwarnings(action="ignore", category=FutureWarning)
# warnings.filterwarnings(action="ignore", category=ImportWarning)
# warnings.filterwarnings(action="ignore", category=DeprecationWarning)

import numpy as np
from copy import deepcopy

from surrogate import benchmarks

from surrogate.base import Individual
from surrogate.selection import selNSGA2, selTournamentDCD
from surrogate.crossover import cxSimulatedBinaryBounded
from surrogate.mutation import mutPolynomialBounded
from surrogate.sampling import samBeta, samUniform
from surrogate.estimator import ANNSurrogate
from surrogate.files import jsonMOEA

from sklearn.preprocessing import StandardScaler

# Xold_ind = [[0., 0., 0., 0.], [1., 1., 1., 1.], [10., 10., 10., 10.]]
# # Yold_obj = [0.0, 1.0, 10.0]
# Yold_obj = [[0., 0.], [1., 1.], [10., 10.]]
# Xnew_ind = [[5., 5., 5., 5.], [-10., -2., -10., -2.]]


import random

random.seed(0.5)


def Population(numPop=4, numVar=10, estimator=benchmarks.zdt6, weights=(-1.0, -1.0)):
    """Population

    :param numPop:
    :param numVar:
    :param estimator:
    :param weights:
    :return:
    """

    Individuals = []
    variables = [
        [0.34238,0.55126,0.16168,0.48371,0.06763,0.44667,0.86263,0.20732,0.69387,0.14216,0.77355,0.01406,0.04584,0.52986,0.43661,0.33434,0.74209,0.10401,0.72842,0.93044,0.60429,0.59564,0.13290,0.67822,0.05134,0.29150,0.67406,0.40855,0.90770,0.11460,0.31785,0.71099,0.54911,0.13772,0.72012,0.57978,0.56174,0.41030,0.59318,0.02373,0.07085,0.38774,0.98321,0.95610,0.60783,0.26427,0.23864,0.38667,0.59478,0.11615,0.90078,0.46229,0.83664,0.11390,0.90762,0.78042,0.63465,0.81593,0.78104,0.43407,0.07091,0.87192,0.89738,0.27897,0.24761,0.94504,0.19522,0.78887,0.53085,0.82305,0.69046,0.63141,0.01483,0.99412,0.42017,0.21348,0.16170,0.69172,0.80659,0.63886,0.95410,0.17915,0.26533,0.30091,0.58426,0.15614,0.17707,0.67962,0.06792,0.23327,0.31763,0.00217,0.27251,0.32120,0.17449,0.05171,0.48460,0.82493,0.27719,0.30186],
        [0.86354,0.05201,0.92801,0.29569,0.87721,0.64880,0.27551,0.19623,0.09760,0.23449,0.00757,0.43299,0.61555,0.53336,0.96767,0.34710,0.80903,0.82793,0.71816,0.97633,0.61393,0.65343,0.97098,0.42527,0.81404,0.64680,0.13768,0.92436,0.37619,0.07402,0.25979,0.28701,0.34837,0.79731,0.80880,0.43086,0.94535,0.56509,0.20545,0.94386,0.39789,0.84186,0.50103,0.08075,0.97736,0.78909,0.00450,0.78641,0.04292,0.47486,0.97810,0.07042,0.11459,0.59133,0.25535,0.82674,0.08913,0.50676,0.34225,0.53017,0.87949,0.55717,0.06603,0.75655,0.84420,0.41342,0.35114,0.64074,0.50797,0.94503,0.66435,0.04266,0.07474,0.66441,0.26592,0.84352,0.16547,0.49778,0.85545,0.07752,0.70118,0.32437,0.56817,0.24237,0.71719,0.65988,0.22632,0.31790,0.48675,0.86751,0.57448,0.42043,0.29419,0.08226,0.16046,0.28985,0.66562,0.14811,0.44042,0.77667],
        [0.91969,0.44927,0.03835,0.43098,0.79035,0.59485,0.40845,0.61248,0.87853,0.55534,0.04745,0.87857,0.75991,0.29952,0.26267,0.26058,0.20349,0.69156,0.33406,0.23523,0.41000,0.86166,0.52943,0.18310,0.60593,0.96086,0.91043,0.95705,0.15646,0.44131,0.89391,0.69190,0.55522,0.09845,0.34478,0.81360,0.59524,0.29984,0.95160,0.79720,0.33984,0.84947,0.46865,0.84490,0.13999,0.68411,0.92793,0.73528,0.30872,0.46048,0.47322,0.51537,0.31971,0.84374,0.93495,0.61703,0.13577,0.40470,0.84247,0.04421,0.34131,0.70754,0.64431,0.44306,0.16737,0.97370,0.47667,0.64699,0.61822,0.98015,0.38152,0.27685,0.06247,0.22531,0.93255,0.93512,0.02240,0.46667,0.34257,0.28409,0.97407,0.64587,0.60096,0.49760,0.41827,0.86877,0.98335,0.87204,0.74038,0.74784,0.02632,0.70023,0.58072,0.09641,0.89199,0.77369,0.87996,0.87224,0.24751,0.97429],
        [0.13993,0.15122,0.28573,0.43139,0.65417,0.46938,0.86108,0.92597,0.74709,0.14420,0.05449,0.51451,0.56960,0.05372,0.00209,0.64311,0.54802,0.92184,0.04323,0.64021,0.17943,0.59804,0.76696,0.86337,0.27164,0.72914,0.11041,0.68976,0.72730,0.75478,0.57193,0.60951,0.26681,0.82851,0.69702,0.81106,0.47750,0.95221,0.58772,0.26501,0.26486,0.47917,0.59469,0.20937,0.11172,0.62145,0.68661,0.03835,0.00669,0.50555,0.67890,0.32771,0.92046,0.12117,0.41331,0.04830,0.39819,0.71502,0.14166,0.55383,0.50429,0.67958,0.57277,0.33917,0.72800,0.31922,0.14091,0.85611,0.89408,0.13871,0.62509,0.75759,0.84096,0.06924,0.65697,0.00145,0.92883,0.83722,0.01857,0.31893,0.04291,0.73191,0.35353,0.18829,0.89315,0.82325,0.11118,0.57182,0.71178,0.24327,0.22988,0.37187,0.06202,0.84425,0.85993,0.03248,0.83203,0.90650,0.08578,0.79938],
        [0.25622,0.68337,0.71353,0.89825,0.23083,0.91845,0.33743,0.71934,0.76041,0.01993,0.47204,0.23828,0.33990,0.96711,0.52421,0.43872,0.55882,0.47906,0.32659,0.60731,0.31930,0.17326,0.73172,0.76329,0.61711,0.50416,0.33392,0.62420,0.34775,0.07851,0.37787,0.21099,0.05401,0.83064,0.80994,0.76269,0.81568,0.03063,0.77810,0.16018,0.89798,0.09088,0.44181,0.26054,0.86087,0.51089,0.18518,0.52363,0.21124,0.37777,0.32621,0.99480,0.23694,0.63256,0.21821,0.76798,0.88812,0.43994,0.39619,0.16088,0.43454,0.02687,0.26855,0.58871,0.51070,0.66748,0.05179,0.04189,0.07335,0.70885,0.71175,0.14206,0.74646,0.15924,0.01326,0.32030,0.06869,0.57787,0.54111,0.07242,0.91853,0.69939,0.82651,0.02291,0.12155,0.70440,0.97397,0.22390,0.46918,0.05982,0.64929,0.32424,0.75642,0.80532,0.67218,0.41296,0.66157,0.00426,0.20435,0.09715],
        [0.33950,0.11129,0.13990,0.92057,0.04839,0.87464,0.88256,0.03211,0.11358,0.77942,0.25365,0.21367,0.01905,0.70306,0.78077,0.44218,0.31915,0.12868,0.75293,0.76212,0.59909,0.79304,0.81316,0.15859,0.91177,0.08790,0.27834,0.46812,0.38897,0.74662,0.89987,0.78176,0.23911,0.29148,0.37972,0.98559,0.35279,0.61138,0.26379,0.05443,0.07976,0.79457,0.03580,0.53814,0.43837,0.05491,0.26370,0.41765,0.23272,0.15754,0.92971,0.12799,0.58959,0.22263,0.32669,0.18999,0.02108,0.38858,0.83878,0.61039,0.52980,0.38479,0.08823,0.07182,0.11034,0.85740,0.57786,0.30981,0.87932,0.05415,0.23808,0.09615,0.38500,0.75464,0.67344,0.78159,0.35520,0.33621,0.27378,0.61473,0.47472,0.18650,0.17610,0.55573,0.85706,0.85894,0.21952,0.92601,0.83456,0.46882,0.04474,0.56210,0.24680,0.30205,0.84857,0.83281,0.96278,0.33119,0.48398,0.68419],
        [0.08243,0.03341,0.22078,0.94716,0.78608,0.48465,0.23130,0.51727,0.02088,0.12268,0.71841,0.46294,0.22383,0.30830,0.18419,0.53082,0.28265,0.93758,0.59146,0.76579,0.67574,0.06593,0.57320,0.76917,0.78406,0.50795,0.95040,0.99856,0.80486,0.96484,0.88043,0.97192,0.87594,0.23267,0.14336,0.97104,0.95933,0.61185,0.94589,0.27082,0.49129,0.04426,0.26725,0.34385,0.24729,0.17311,0.70698,0.53915,0.92952,0.29853,0.55584,0.16898,0.50524,0.01853,0.97205,0.32129,0.56449,0.63629,0.63298,0.79616,0.16276,0.65904,0.15219,0.38082,0.81187,0.70755,0.26634,0.53644,0.53551,0.39307,0.12043,0.77142,0.51372,0.24156,0.44028,0.17700,0.29354,0.67501,0.03018,0.25163,0.07918,0.11981,0.61453,0.45503,0.89952,0.61987,0.23355,0.03523,0.99769,0.21151,0.19265,0.20895,0.79386,0.19159,0.60022,0.72738,0.30998,0.91191,0.90247,0.77567],
        [0.90767,0.68194,0.64657,0.71068,0.89695,0.54602,0.58751,0.84399,0.17644,0.02304,0.61269,0.03657,0.67512,0.35543,0.15995,0.18302,0.52408,0.03353,0.53351,0.76008,0.42700,0.86507,0.57826,0.26565,0.14384,0.18752,0.43051,0.85559,0.11147,0.03764,0.95909,0.86539,0.87697,0.95579,0.33884,0.55849,0.38692,0.82809,0.16079,0.54912,0.22955,0.85218,0.11460,0.66511,0.64945,0.83646,0.08843,0.24688,0.36118,0.10473,0.49191,0.49398,0.17425,0.15437,0.98346,0.38569,0.14302,0.51544,0.50596,0.90466,0.24108,0.17373,0.73275,0.59064,0.10658,0.89165,0.69076,0.40214,0.61042,0.65772,0.01952,0.21124,0.85153,0.82110,0.79348,0.98321,0.17420,0.08691,0.74204,0.41192,0.82141,0.43549,0.52165,0.35900,0.10016,0.77085,0.74525,0.27992,0.69122,0.64209,0.10021,0.83897,0.53332,0.15176,0.17092,0.67785,0.90428,0.37991,0.70872,0.25275],
        [0.98004,0.56153,0.57236,0.89949,0.55672,0.10744,0.24179,0.93445,0.65553,0.68621,0.88867,0.47840,0.10266,0.58013,0.64518,0.29286,0.09926,0.50435,0.48831,0.07047,0.72079,0.28114,0.82060,0.84696,0.12834,0.83673,0.95704,0.09121,0.91362,0.22020,0.65512,0.45692,0.29391,0.50649,0.23899,0.78426,0.39437,0.75408,0.59646,0.14129,0.26679,0.84833,0.27972,0.38612,0.31095,0.95805,0.96893,0.71682,0.31423,0.46306,0.21191,0.48344,0.75630,0.24330,0.94847,0.53725,0.42761,0.16256,0.07606,0.24678,0.03949,0.03740,0.32526,0.54502,0.71044,0.89843,0.60514,0.28887,0.18410,0.50928,0.49754,0.55423,0.47626,0.71970,0.66170,0.91486,0.53976,0.73714,0.69275,0.42089,0.51907,0.87809,0.34634,0.68946,0.33329,0.27241,0.62474,0.08902,0.56677,0.52112,0.05370,0.58558,0.79542,0.78725,0.47196,0.96997,0.27196,0.52723,0.71548,0.99552],
        [0.22041,0.40971,0.72413,0.70128,0.33304,0.45778,0.41855,0.35549,0.88068,0.12306,0.86490,0.98607,0.37495,0.24727,0.10240,0.23160,0.08114,0.99119,0.43979,0.33290,0.36012,0.42576,0.06832,0.55192,0.03877,0.27849,0.40438,0.80528,0.29203,0.98320,0.24369,0.33412,0.26282,0.84144,0.80492,0.03584,0.89118,0.44048,0.60209,0.54040,0.95594,0.36422,0.61346,0.46270,0.81559,0.83517,0.68711,0.95067,0.47963,0.65661,0.89107,0.60445,0.28701,0.81647,0.77683,0.92579,0.48309,0.22421,0.61680,0.90926,0.57756,0.95019,0.86558,0.35621,0.21900,0.32563,0.83333,0.00621,0.62459,0.11625,0.69426,0.03365,0.43159,0.68475,0.16053,0.58528,0.87686,0.01668,0.54485,0.92991,0.12927,0.83253,0.33480,0.36496,0.50699,0.00349,0.83393,0.73147,0.74555,0.30974,0.77243,0.47741,0.76345,0.30704,0.95874,0.26513,0.57070,0.20817,0.19472,0.00865],
        [0.71480,0.19853,0.26717,0.47509,0.32239,0.60334,0.67499,0.84484,0.20563,0.59455,0.44270,0.77465,0.89914,0.85156,0.07589,0.13518,0.67195,0.67987,0.10000,0.20741,0.97206,0.86580,0.56785,0.45750,0.04110,0.14148,0.44048,0.88397,0.48170,0.49492,0.26016,0.50386,0.67592,0.13398,0.33251,0.01060,0.45089,0.87929,0.09367,0.59020,0.51107,0.93064,0.98182,0.97036,0.09665,0.41824,0.56611,0.95110,0.73197,0.46268,0.11762,0.01549,0.77333,0.58979,0.34250,0.89528,0.38576,0.02030,0.66269,0.08606,0.82137,0.79753,0.29928,0.42905,0.74887,0.23799,0.63293,0.80107,0.87426,0.21059,0.09492,0.82989,0.48352,0.44769,0.94969,0.04664,0.65729,0.44202,0.11694,0.20476,0.54089,0.24521,0.32286,0.18621,0.67467,0.98601,0.30863,0.46143,0.60758,0.88742,0.24326,0.00026,0.85238,0.30760,0.28433,0.05028,0.48756,0.64862,0.84634,0.54704],
        [0.73619,0.25922,0.16925,0.21597,0.95453,0.83925,0.95428,0.54666,0.22385,0.09894,0.22417,0.29050,0.71564,0.30417,0.35540,0.68517,0.15286,0.24721,0.57402,0.59751,0.20248,0.52241,0.00058,0.80079,0.34587,0.99043,0.34535,0.23636,0.99084,0.96583,0.23279,0.72682,0.04692,0.21868,0.17569,0.19078,0.48613,0.18530,0.13256,0.95558,0.36961,0.04323,0.14496,0.27933,0.18210,0.65562,0.34229,0.42402,0.78786,0.19189,0.05071,0.18011,0.40938,0.11882,0.30584,0.60007,0.72350,0.85244,0.35653,0.38918,0.77009,0.97661,0.46168,0.09646,0.92769,0.34529,0.11210,0.88876,0.66684,0.03027,0.05260,0.94270,0.92006,0.93250,0.65450,0.96182,0.05036,0.04944,0.81834,0.81641,0.46670,0.60847,0.44034,0.58623,0.38435,0.95550,0.16369,0.38360,0.19917,0.95864,0.63624,0.36588,0.22855,0.47259,0.02913,0.28834,0.68326,0.19535,0.69896,0.78887],
        [0.89235,0.31249,0.51641,0.77011,0.58867,0.34448,0.21607,0.56110,0.61896,0.67768,0.85914,0.02558,0.63914,0.95737,0.61925,0.94232,0.76699,0.30183,0.99697,0.18738,0.43211,0.65662,0.59294,0.08637,0.88008,0.67421,0.60733,0.05867,0.82276,0.81290,0.00653,0.77067,0.75882,0.87103,0.47394,0.27041,0.39275,0.58849,0.73584,0.23723,0.49082,0.15999,0.61580,0.04132,0.23647,0.48095,0.34778,0.26028,0.86867,0.21543,0.77443,0.15832,0.83597,0.20731,0.21363,0.13265,0.01511,0.08754,0.12812,0.19973,0.00901,0.80706,0.07499,0.92797,0.78329,0.55877,0.23258,0.48954,0.25954,0.79263,0.90372,0.99957,0.47252,0.18355,0.45917,0.29153,0.87836,0.59624,0.67553,0.46216,0.22644,0.41902,0.92447,0.11293,0.66342,0.27166,0.93713,0.06615,0.30806,0.86274,0.37205,0.62810,0.82240,0.19445,0.04336,0.95203,0.61205,0.60336,0.54192,0.08490],
        [0.57103,0.26741,0.93530,0.90874,0.40949,0.52810,0.38948,0.45402,0.79837,0.62215,0.66332,0.21428,0.74101,0.11271,0.05674,0.86232,0.38355,0.14742,0.59693,0.18001,0.07853,0.64811,0.18589,0.88465,0.37274,0.38352,0.44635,0.65883,0.49729,0.61803,0.07980,0.49147,0.54400,0.20664,0.58461,0.36854,0.17039,0.36797,0.43217,0.21857,0.12570,0.71444,0.97034,0.49819,0.61916,0.29503,0.98667,0.02651,0.95219,0.73293,0.00859,0.09921,0.66614,0.80886,0.64290,0.52903,0.08036,0.43353,0.03789,0.19738,0.70781,0.66082,0.49377,0.21126,0.52421,0.87856,0.65832,0.30132,0.80553,0.31066,0.37170,0.45477,0.51930,0.04790,0.30109,0.09178,0.40868,0.42481,0.05532,0.64175,0.27328,0.30284,0.30736,0.98022,0.27079,0.50059,0.95627,0.85202,0.21828,0.03932,0.34398,0.14868,0.74104,0.98045,0.52235,0.35443,0.34678,0.50598,0.19638,0.75440],
        [0.25056,0.45746,0.27204,0.14151,0.72298,0.51300,0.83907,0.34107,0.86590,0.01479,0.99437,0.43781,0.02789,0.81732,0.47529,0.24320,0.18428,0.71268,0.09595,0.85631,0.19085,0.22898,0.64614,0.09862,0.25745,0.97537,0.34175,0.78771,0.45468,0.66054,0.16612,0.65715,0.94555,0.19786,0.16048,0.55054,0.58409,0.98635,0.90705,0.36497,0.61917,0.94740,0.58398,0.17720,0.72530,0.28751,0.63484,0.85027,0.58759,0.67823,0.93676,0.16133,0.22815,0.74623,0.02462,0.12510,0.34120,0.44040,0.37850,0.88329,0.32690,0.92764,0.18691,0.75297,0.94407,0.19058,0.17144,0.99173,0.99470,0.70378,0.55679,0.75669,0.84219,0.81789,0.14884,0.26099,0.25714,0.64297,0.46124,0.08342,0.45272,0.16860,0.22452,0.02078,0.97392,0.69440,0.73924,0.78764,0.83823,0.20797,0.38813,0.74786,0.42887,0.25890,0.71968,0.50395,0.67954,0.82446,0.62233,0.85574],
        [0.81061,0.03060,0.97848,0.35664,0.13428,0.91363,0.28595,0.63545,0.51073,0.52618,0.84589,0.51830,0.57334,0.93271,0.24788,0.55116,0.46959,0.76416,0.46899,0.74412,0.03096,0.75178,0.74606,0.38774,0.87937,0.97234,0.47634,0.71208,0.37285,0.25243,0.73544,0.37638,0.83288,0.30620,0.88642,0.98508,0.54345,0.94574,0.50345,0.43106,0.50619,0.55651,0.90985,0.83152,0.16586,0.57336,0.40500,0.81769,0.95190,0.54850,0.82114,0.89449,0.12874,0.23408,0.71990,0.58675,0.46352,0.22388,0.95216,0.96201,0.36581,0.26281,0.17272,0.56275,0.66015,0.43793,0.09337,0.24118,0.66795,0.32737,0.44454,0.53354,0.56366,0.21094,0.30739,0.08350,0.09971,0.99121,0.61657,0.91547,0.86581,0.16460,0.99336,0.67888,0.20877,0.11369,0.36273,0.13056,0.37129,0.69558,0.63174,0.26505,0.94880,0.30217,0.10748,0.39643,0.21067,0.20856,0.96060,0.38096],
        [0.55912,0.71204,0.45033,0.06211,0.27684,0.77174,0.10577,0.88025,0.41034,0.23443,0.60914,0.50800,0.34660,0.86859,0.78909,0.25058,0.48331,0.49648,0.22694,0.51583,0.31158,0.06270,0.79394,0.25349,0.07898,0.35204,0.02831,0.19358,0.14633,0.78008,0.02387,0.73930,0.77035,0.35521,0.17112,0.94938,0.68310,0.68018,0.64906,0.50339,0.29828,0.32507,0.09248,0.05112,0.23160,0.12897,0.47827,0.37266,0.40465,0.38419,0.02761,0.80374,0.35466,0.65782,0.87721,0.12453,0.42782,0.24773,0.54101,0.36496,0.53535,0.29723,0.74840,0.84047,0.95283,0.61872,0.21095,0.39060,0.88217,0.93889,0.88521,0.00362,0.15744,0.44545,0.32519,0.57248,0.45603,0.85355,0.38011,0.11802,0.50683,0.57659,0.77102,0.62638,0.85751,0.32345,0.14704,0.32122,0.59723,0.92300,0.71492,0.94646,0.01261,0.46333,0.28141,0.21156,0.55536,0.23285,0.23847,0.46151],
        [0.68316,0.12165,0.10252,0.19039,0.26287,0.20488,0.86955,0.91234,0.60964,0.87649,0.83793,0.57240,0.60030,0.02433,0.22472,0.42094,0.55331,0.86054,0.01814,0.18552,0.19958,0.03049,0.51478,0.18835,0.51241,0.12315,0.82022,0.99594,0.39207,0.65018,0.29839,0.03516,0.82317,0.80295,0.24342,0.08113,0.66130,0.47874,0.69474,0.71806,0.82766,0.83903,0.97845,0.52075,0.24188,0.89937,0.36326,0.63296,0.63390,0.25200,0.93878,0.82098,0.27784,0.59438,0.07303,0.97444,0.39655,0.58560,0.28806,0.83361,0.86314,0.13119,0.61414,0.07767,0.05968,0.99073,0.41533,0.20434,0.50615,0.05427,0.48318,0.72790,0.53023,0.32918,0.56802,0.75086,0.78426,0.00325,0.89822,0.23609,0.35353,0.59113,0.57283,0.20655,0.65605,0.71521,0.15894,0.68441,0.08990,0.48521,0.91155,0.36294,0.56115,0.58938,0.09017,0.05229,0.49034,0.01904,0.20104,0.02002],
        [0.17785,0.37152,0.73669,0.99537,0.50029,0.28723,0.38087,0.12005,0.11158,0.90957,0.81944,0.09855,0.21371,0.36282,0.22499,0.37104,0.39032,0.47282,0.67924,0.80246,0.12984,0.29735,0.71710,0.70583,0.30555,0.41287,0.17366,0.41902,0.46016,0.53919,0.49904,0.53729,0.41534,0.66140,0.13502,0.74100,0.97112,0.66879,0.00859,0.37343,0.69595,0.22600,0.97267,0.79890,0.21123,0.73678,0.46263,0.19798,0.97868,0.82492,0.51043,0.48935,0.13937,0.24869,0.61358,0.26099,0.00630,0.43202,0.79162,0.93953,0.85702,0.08646,0.53566,0.58427,0.01682,0.06850,0.22426,0.64447,0.01728,0.07925,0.73567,0.22526,0.86777,0.35323,0.38530,0.28334,0.31793,0.12498,0.29079,0.87787,0.42042,0.76723,0.90596,0.66281,0.39403,0.98525,0.23807,0.91165,0.76680,0.76456,0.94373,0.88153,0.83446,0.55180,0.26517,0.71154,0.29293,0.75899,0.53310,0.30103],
        [0.17316,0.77116,0.48305,0.24316,0.03348,0.09259,0.77840,0.01435,0.06064,0.98440,0.15643,0.92260,0.86562,0.80454,0.05366,0.40412,0.74704,0.48761,0.30736,0.21576,0.03830,0.10516,0.37285,0.27140,0.02694,0.25587,0.60423,0.99598,0.23556,0.03962,0.78652,0.06361,0.98449,0.14298,0.89419,0.18567,0.59986,0.52512,0.79085,0.85445,0.23272,0.52584,0.97255,0.29510,0.12145,0.04351,0.70195,0.13795,0.99094,0.10857,0.32827,0.73837,0.38857,0.50584,0.24238,0.22412,0.39510,0.05772,0.04894,0.36567,0.16282,0.21223,0.11582,0.83805,0.58895,0.58515,0.44401,0.04340,0.36263,0.86546,0.76903,0.47597,0.37653,0.82685,0.40699,0.97273,0.01437,0.64085,0.80224,0.21410,0.74023,0.82354,0.58593,0.90070,0.21171,0.90573,0.04070,0.59138,0.21732,0.04800,0.45078,0.41759,0.71237,0.66919,0.24865,0.27622,0.95885,0.65859,0.13644,0.85682],
        [0.99079,0.76862,0.30495,0.02964,0.14578,0.17939,0.30278,0.82805,0.56023,0.65777,0.06794,0.15815,0.36488,0.56269,0.70153,0.38765,0.15214,0.42317,0.53920,0.85067,0.94347,0.62186,0.64245,0.14934,0.89607,0.21924,0.33815,0.74205,0.35866,0.48570,0.05061,0.71314,0.90504,0.26311,0.19715,0.21553,0.51131,0.10271,0.84977,0.13577,0.99439,0.58470,0.89085,0.76706,0.96966,0.77886,0.61396,0.33772,0.92928,0.55483,0.27037,0.77243,0.86035,0.84058,0.35777,0.68610,0.20860,0.78498,0.79700,0.66926,0.11245,0.38283,0.65669,0.50268,0.92584,0.38895,0.70056,0.91135,0.60882,0.52395,0.15602,0.07596,0.20396,0.65935,0.09822,0.09414,0.95842,0.34241,0.05946,0.57437,0.19055,0.14686,0.82675,0.29887,0.13125,0.08421,0.16939,0.09036,0.41222,0.84458,0.95670,0.14198,0.52825,0.44530,0.02237,0.07767,0.45579,0.87877,0.54015,0.54314],
        [0.97096,0.50635,0.72924,0.41277,0.58476,0.96420,0.59228,0.33946,0.90697,0.79306,0.25682,0.79662,0.57857,0.48748,0.18296,0.72032,0.45978,0.67701,0.59550,0.83160,0.81626,0.39035,0.29324,0.37050,0.91932,0.46612,0.30156,0.81494,0.89774,0.11085,0.83537,0.94290,0.49479,0.58944,0.62959,0.59867,0.59409,0.75804,0.59062,0.32349,0.40950,0.94716,0.81047,0.00882,0.60154,0.81713,0.36080,0.91826,0.55317,0.34817,0.93151,0.69825,0.74162,0.23764,0.94854,0.75664,0.10791,0.01495,0.72272,0.76007,0.60963,0.10198,0.33413,0.82043,0.05949,0.03782,0.09652,0.63603,0.17197,0.20597,0.28676,0.37071,0.43619,0.18560,0.94785,0.29860,0.68446,0.47761,0.37788,0.62055,0.79391,0.44394,0.96583,0.25217,0.78182,0.17512,0.72399,0.74059,0.21255,0.74714,0.30820,0.81523,0.44686,0.77474,0.88148,0.82279,0.56359,0.14395,0.82095,0.35344],
        [0.94169,0.69000,0.66439,0.64803,0.08492,0.98596,0.64830,0.78516,0.76449,0.52975,0.87565,0.37917,0.06793,0.34182,0.55793,0.82691,0.27181,0.55054,0.44210,0.22841,0.03433,0.26039,0.00303,0.32209,0.92697,0.01019,0.36708,0.18380,0.05019,0.37951,0.80022,0.03362,0.85724,0.92301,0.29711,0.62769,0.15315,0.86454,0.50895,0.77647,0.50576,0.03698,0.07615,0.22079,0.15577,0.49123,0.95845,0.04375,0.93487,0.25857,0.07963,0.11204,0.06922,0.31920,0.94174,0.75600,0.91195,0.31157,0.84662,0.83951,0.70872,0.04873,0.53450,0.34614,0.36624,0.75863,0.18633,0.56910,0.65077,0.02777,0.77955,0.64088,0.38895,0.44713,0.34432,0.69323,0.08127,0.94590,0.58785,0.86508,0.47096,0.56938,0.96759,0.18743,0.54768,0.99014,0.47357,0.65850,0.49411,0.81118,0.06638,0.95181,0.58598,0.93287,0.45352,0.52285,0.57362,0.15224,0.23991,0.54823],
        [0.20440,0.48138,0.98317,0.27243,0.56641,0.91140,0.30131,0.53347,0.16123,0.89782,0.05830,0.56769,0.57549,0.41714,0.35178,0.44565,0.35165,0.10096,0.88198,0.44271,0.04768,0.45871,0.85896,0.32761,0.75416,0.71177,0.96451,0.93158,0.02897,0.86150,0.99496,0.63761,0.18540,0.74232,0.18113,0.75189,0.10657,0.49830,0.33070,0.61978,0.59537,0.94797,0.12760,0.88974,0.44977,0.33990,0.24518,0.00189,0.45459,0.26955,0.83633,0.65687,0.94342,0.69838,0.01792,0.58194,0.15929,0.97645,0.66350,0.61307,0.78329,0.63360,0.81559,0.05411,0.33612,0.23088,0.55180,0.19428,0.58401,0.93294,0.80976,0.42045,0.91988,0.43025,0.28596,0.67313,0.14820,0.76124,0.34264,0.86253,0.30409,0.50291,0.75666,0.75374,0.18403,0.60502,0.81337,0.44349,0.30189,0.89982,0.14198,0.26142,0.32450,0.99774,0.02417,0.12270,0.59437,0.17254,0.69518,0.90263],
        [0.54055,0.72710,0.54706,0.43253,0.11922,0.70147,0.85320,0.69659,0.06216,0.64785,0.54695,0.57510,0.16556,0.88861,0.04037,0.96647,0.67489,0.93834,0.07072,0.45982,0.79107,0.14056,0.87609,0.97383,0.07900,0.13947,0.55725,0.53136,0.49007,0.10813,0.81551,0.19461,0.92235,0.36776,0.05908,0.64113,0.41765,0.71760,0.03177,0.37262,0.37030,0.02049,0.00759,0.94044,0.20416,0.87226,0.44175,0.48608,0.61429,0.59383,0.49938,0.82592,0.87179,0.53226,0.81166,0.04696,0.40724,0.84918,0.15308,0.92829,0.12713,0.73308,0.12985,0.90490,0.31335,0.21973,0.93357,0.36599,0.30042,0.43389,0.15999,0.58436,0.58879,0.76058,0.17790,0.03666,0.49262,0.84999,0.28230,0.28971,0.51863,0.54160,0.09114,0.87951,0.17832,0.02252,0.56644,0.32876,0.84705,0.40889,0.83386,0.07649,0.87119,0.33774,0.12718,0.28337,0.84785,0.08759,0.32888,0.22427],
        [0.52879,0.01298,0.02109,0.39246,0.09188,0.36320,0.38653,0.42839,0.77872,0.32815,0.77729,0.58743,0.50932,0.83925,0.07124,0.30994,0.28246,0.86093,0.14598,0.43635,0.20914,0.44569,0.05785,0.81371,0.20261,0.91403,0.86690,0.29004,0.30400,0.64514,0.28615,0.05796,0.94521,0.25744,0.66819,0.08719,0.27622,0.25731,0.51281,0.67968,0.41147,0.60744,0.77552,0.73950,0.35878,0.45824,0.24130,0.54136,0.70475,0.21567,0.21095,0.38920,0.30434,0.97546,0.95853,0.34719,0.81310,0.29321,0.72949,0.22792,0.09830,0.99150,0.41568,0.89227,0.67183,0.84023,0.05538,0.76108,0.60328,0.51411,0.70601,0.41076,0.53546,0.51415,0.49608,0.05095,0.80847,0.94973,0.80893,0.24277,0.24537,0.58093,0.91671,0.34747,0.08391,0.64559,0.39620,0.63941,0.21269,0.56626,0.93294,0.66736,0.84344,0.52099,0.97056,0.63553,0.07144,0.45908,0.23967,0.49884],
        [0.47355,0.24702,0.41131,0.26956,0.05899,0.71372,0.39029,0.44852,0.54435,0.39255,0.95601,0.03486,0.27231,0.16648,0.89353,0.36181,0.53939,0.85922,0.34421,0.38696,0.98757,0.92098,0.48404,0.05018,0.97868,0.63746,0.28283,0.45662,0.98681,0.59837,0.93047,0.68510,0.28947,0.61101,0.15550,0.41799,0.29836,0.84417,0.74832,0.04762,0.44652,0.48138,0.78801,0.46689,0.94906,0.47796,0.87402,0.97802,0.66786,0.78204,0.56002,0.03135,0.52510,0.05343,0.72594,0.03963,0.58283,0.44514,0.70964,0.47011,0.75232,0.06575,0.51159,0.33901,0.61916,0.03356,0.00339,0.45363,0.40988,0.71378,0.60245,0.17578,0.05975,0.11789,0.66750,0.74075,0.29883,0.73970,0.22532,0.36951,0.30470,0.34080,0.82052,0.29364,0.76585,0.04868,0.78227,0.85947,0.86374,0.12515,0.37936,0.62881,0.44264,0.52727,0.32541,0.30468,0.46881,0.55223,0.16284,0.23776],
        [0.16476,0.02081,0.88507,0.47726,0.62916,0.56782,0.51753,0.02478,0.83384,0.51177,0.62102,0.19190,0.58898,0.63553,0.87018,0.65111,0.23270,0.79118,0.10407,0.10693,0.24798,0.94053,0.01019,0.65190,0.46200,0.09549,0.40657,0.48890,0.02595,0.82579,0.04016,0.51167,0.91448,0.75976,0.62783,0.34268,0.11575,0.19550,0.40692,0.67488,0.24070,0.13817,0.86812,0.98266,0.83647,0.88795,0.09048,0.59167,0.10978,0.98114,0.01235,0.42961,0.48768,0.46467,0.66601,0.35798,0.19116,0.33025,0.49942,0.89333,0.09995,0.39248,0.78770,0.18398,0.28209,0.88926,0.03353,0.85982,0.22133,0.54047,0.06749,0.31885,0.34695,0.19448,0.58486,0.07497,0.49021,0.19625,0.61697,0.36086,0.51777,0.70014,0.04113,0.42959,0.20548,0.54135,0.54094,0.32795,0.48712,0.18080,0.75284,0.67440,0.32526,0.49026,0.40609,0.13704,0.01768,0.12287,0.97818,0.13563],
        [0.93810,0.64143,0.51399,0.29784,0.73312,0.04797,0.14782,0.57685,0.91975,0.34667,0.25820,0.07425,0.05722,0.52257,0.46768,0.11911,0.24962,0.80678,0.00847,0.61850,0.07629,0.81891,0.05196,0.61719,0.40615,0.19085,0.44949,0.53559,0.68827,0.64769,0.68310,0.18191,0.54495,0.07900,0.43092,0.90147,0.53771,0.64645,0.74816,0.42352,0.44989,0.83301,0.56170,0.61121,0.74982,0.71701,0.30701,0.90317,0.64643,0.23799,0.51833,0.51603,0.99976,0.21719,0.06751,0.02407,0.95465,0.39121,0.29870,0.55682,0.19121,0.13440,0.45257,0.85079,0.84619,0.78366,0.89638,0.39583,0.55869,0.76312,0.79819,0.02887,0.12036,0.73856,0.33273,0.55951,0.03261,0.54415,0.72529,0.68366,0.15031,0.71638,0.48649,0.69495,0.58100,0.44395,0.62493,0.05917,0.54789,0.62044,0.22848,0.73863,0.65147,0.50913,0.97368,0.20469,0.97204,0.71110,0.32051,0.70258],
        [0.59854,0.36085,0.69661,0.19247,0.96441,0.60786,0.78490,0.39287,0.87240,0.91126,0.95182,0.68882,0.63820,0.81728,0.76389,0.04626,0.37887,0.34400,0.93028,0.27090,0.05885,0.70750,0.49349,0.89902,0.61826,0.92770,0.08959,0.80240,0.41877,0.18281,0.24384,0.68210,0.04081,0.85600,0.61428,0.32945,0.84125,0.70472,0.45007,0.16986,0.42930,0.19939,0.13084,0.68803,0.61697,0.34676,0.97490,0.48023,0.39686,0.17475,0.87833,0.62101,0.37050,0.23748,0.66981,0.64431,0.12582,0.84296,0.22610,0.04925,0.16148,0.10987,0.51443,0.51153,0.67106,0.15046,0.04961,0.46131,0.55931,0.78281,0.50762,0.23051,0.40783,0.39379,0.58702,0.35957,0.99929,0.87892,0.49021,0.92879,0.26475,0.60611,0.34386,0.11610,0.65610,0.69993,0.27760,0.21955,0.56560,0.72716,0.69312,0.63630,0.76740,0.19146,0.95695,0.14521,0.35699,0.15295,0.04504,0.87325],
        [0.40902,0.26362,0.27273,0.21203,0.14590,0.78604,0.92254,0.93414,0.60174,0.48990,0.02858,0.44709,0.90124,0.42539,0.87791,0.24196,0.00245,0.62935,0.62120,0.56707,0.04484,0.85494,0.92519,0.79729,0.32605,0.83960,0.80649,0.69602,0.01381,0.12625,0.64107,0.01505,0.99741,0.14733,0.61178,0.92094,0.33144,0.37066,0.15618,0.07574,0.98884,0.68824,0.24185,0.73291,0.84893,0.02721,0.17196,0.54631,0.82854,0.64886,0.32806,0.57250,0.08819,0.84525,0.23401,0.42869,0.42067,0.27055,0.27539,0.27695,0.22607,0.08647,0.47197,0.45501,0.53821,0.84260,0.36957,0.99237,0.70955,0.34955,0.39690,0.21398,0.07386,0.04564,0.45261,0.53387,0.27038,0.91523,0.17748,0.44564,0.09641,0.68560,0.23409,0.14530,0.80716,0.55977,0.39516,0.29116,0.45257,0.92464,0.08581,0.65301,0.78762,0.13435,0.03106,0.83007,0.92651,0.20103,0.93736,0.55767],
        [0.34907,0.39813,0.89608,0.74015,0.67969,0.90609,0.89564,0.84890,0.56897,0.72335,0.07088,0.88995,0.24089,0.62666,0.88464,0.14382,0.93043,0.11289,0.94594,0.74541,0.82798,0.46567,0.27870,0.56420,0.97850,0.22733,0.56664,0.46149,0.15567,0.46051,0.21769,0.48777,0.46998,0.84025,0.08035,0.61842,0.65415,0.82099,0.41851,0.23077,0.16423,0.16523,0.65603,0.90096,0.76930,0.39483,0.17731,0.97379,0.83175,0.51193,0.66106,0.61395,0.42094,0.88387,0.38768,0.58135,0.98146,0.56238,0.44305,0.66293,0.33298,0.92683,0.32146,0.36020,0.18542,0.20487,0.76155,0.55826,0.67050,0.49146,0.41828,0.10948,0.67869,0.76792,0.76351,0.72559,0.34580,0.90958,0.14221,0.50741,0.54515,0.05394,0.68195,0.06291,0.22282,0.34601,0.92153,0.85131,0.71131,0.94333,0.04350,0.46621,0.73954,0.46314,0.39079,0.77177,0.39058,0.90153,0.25958,0.30813],
        [0.62672,0.70082,0.60870,0.16808,0.23185,0.68789,0.21942,0.14456,0.55365,0.94992,0.54993,0.33503,0.02315,0.59711,0.21138,0.71198,0.01509,0.15147,0.87981,0.70155,0.90633,0.38045,0.48608,0.70811,0.71939,0.01090,0.24631,0.80809,0.20223,0.04285,0.83476,0.79055,0.73887,0.64265,0.27572,0.58973,0.35041,0.77250,0.94531,0.31428,0.35326,0.30826,0.77567,0.90930,0.06788,0.79289,0.61230,0.23389,0.98202,0.26187,0.32469,0.61899,0.66211,0.92289,0.67201,0.71954,0.03341,0.22340,0.44385,0.57042,0.35745,0.64255,0.54257,0.62712,0.42267,0.46862,0.02956,0.13109,0.02367,0.79710,0.67749,0.33775,0.30129,0.37686,0.59288,0.21973,0.73735,0.25843,0.72342,0.96473,0.63479,0.12516,0.76554,0.26727,0.26613,0.51499,0.53451,0.50950,0.36806,0.45900,0.08403,0.94939,0.26378,0.20429,0.57922,0.34615,0.97261,0.69114,0.62029,0.90954],
        [0.56397,0.81958,0.53010,0.85069,0.68330,0.84477,0.50130,0.30968,0.10696,0.95630,0.70487,0.57648,0.92035,0.51747,0.09291,0.65533,0.27265,0.48737,0.89210,0.41110,0.17516,0.47488,0.69163,0.68919,0.56864,0.42794,0.89518,0.55723,0.75317,0.92470,0.40257,0.94518,0.98979,0.15337,0.56630,0.28394,0.09152,0.12232,0.33274,0.76634,0.35045,0.65615,0.13799,0.59276,0.10230,0.57187,0.51692,0.19208,0.55980,0.31353,0.98111,0.78791,0.78153,0.45962,0.25744,0.53883,0.50117,0.34341,0.07097,0.64604,0.97607,0.03320,0.28879,0.91401,0.75577,0.37125,0.17792,0.30133,0.20463,0.72469,0.40965,0.83820,0.02648,0.18081,0.87809,0.01939,0.58686,0.05821,0.53715,0.36255,0.99791,0.08565,0.90902,0.86152,0.05289,0.50114,0.13508,0.66231,0.73823,0.71957,0.50296,0.33380,0.30575,0.22277,0.84686,0.76151,0.74779,0.36016,0.06347,0.44852],
        [0.43453,0.09123,0.44904,0.68083,0.73945,0.76147,0.05425,0.15749,0.44426,0.13303,0.72078,0.76287,0.96869,0.96023,0.59290,0.26705,0.10731,0.83850,0.63383,0.70334,0.50226,0.04561,0.13046,0.75006,0.00080,0.94471,0.14141,0.82885,0.81935,0.21989,0.76840,0.93578,0.93170,0.22918,0.32682,0.15906,0.96913,0.04128,0.29274,0.20941,0.28152,0.87881,0.08122,0.22581,0.51330,0.90224,0.93383,0.30321,0.03145,0.56399,0.59247,0.58843,0.05192,0.19261,0.68570,0.63976,0.66650,0.50296,0.69360,0.24419,0.30769,0.84893,0.09041,0.11631,0.78511,0.00437,0.44966,0.48276,0.92914,0.02832,0.56853,0.43279,0.93932,0.94064,0.17292,0.84798,0.02615,0.00779,0.37092,0.29621,0.77946,0.53873,0.76922,0.85107,0.82076,0.89393,0.56250,0.34444,0.32010,0.32380,0.02045,0.45565,0.33832,0.51170,0.96064,0.36903,0.07152,0.42086,0.15506,0.87890],
        [0.27715,0.59838,0.44347,0.53164,0.75962,0.63020,0.00165,0.89270,0.01890,0.79951,0.36907,0.77168,0.97769,0.79895,0.68187,0.27975,0.22400,0.14288,0.28619,0.68976,0.94182,0.50256,0.99466,0.97647,0.40910,0.52754,0.12467,0.08539,0.65896,0.37684,0.84289,0.55045,0.93156,0.49167,0.34100,0.21447,0.61155,0.16664,0.97026,0.55234,0.40921,0.52244,0.05266,0.70424,0.58510,0.47773,0.23701,0.43351,0.01745,0.65303,0.87234,0.25267,0.15152,0.69529,0.30216,0.09848,0.97292,0.75446,0.35265,0.00954,0.76542,0.54436,0.86693,0.84482,0.73103,0.81777,0.31330,0.60930,0.73606,0.64360,0.90770,0.32071,0.36253,0.82286,0.38000,0.62631,0.35414,0.97097,0.42741,0.80443,0.78028,0.35017,0.67424,0.15064,0.54764,0.72050,0.80299,0.36333,0.82943,0.56074,0.37470,0.92345,0.44420,0.14448,0.99017,0.03119,0.34612,0.77414,0.76073,0.28575],
        [0.51523,0.85503,0.61256,0.30435,0.01012,0.71521,0.03159,0.68458,0.02338,0.04730,0.66440,0.41813,0.54250,0.71788,0.86115,0.43435,0.67485,0.55234,0.77785,0.59542,0.41255,0.09979,0.31395,0.60338,0.48230,0.68846,0.98285,0.10798,0.70257,0.18153,0.83796,0.30276,0.00711,0.56288,0.94024,0.58613,0.59378,0.90859,0.09361,0.34227,0.09847,0.69350,0.29559,0.67511,0.26822,0.52625,0.16192,0.48445,0.02587,0.42185,0.20449,0.16967,0.15298,0.25054,0.49605,0.72030,0.05321,0.88719,0.99592,0.16613,0.44467,0.20733,0.45134,0.69381,0.10406,0.07126,0.92813,0.10584,0.37174,0.19077,0.79591,0.89317,0.59661,0.31875,0.42561,0.02828,0.48663,0.27038,0.47230,0.63687,0.85681,0.94872,0.00302,0.57564,0.49606,0.48200,0.30600,0.02488,0.94252,0.54520,0.08259,0.66365,0.91702,0.98517,0.38024,0.07243,0.92358,0.56428,0.27572,0.80730],
        [0.85468,0.07190,0.59885,0.11722,0.08941,0.52907,0.46975,0.25121,0.47656,0.57447,0.56889,0.36815,0.46654,0.09908,0.22605,0.25471,0.14397,0.26441,0.76877,0.05708,0.72486,0.38904,0.28591,0.44015,0.28572,0.28266,0.78742,0.19104,0.39469,0.82762,0.87852,0.52682,0.46850,0.19698,0.83482,0.54002,0.62183,0.17828,0.54357,0.39568,0.89346,0.96196,0.28311,0.30156,0.70960,0.68776,0.33043,0.67667,0.96944,0.09206,0.57270,0.41410,0.77814,0.27934,0.28364,0.31113,0.55832,0.74396,0.35863,0.93116,0.31248,0.28060,0.94711,0.66843,0.06317,0.96832,0.40348,0.72473,0.90582,0.99074,0.60417,0.07235,0.57504,0.44500,0.77119,0.75637,0.93331,0.35403,0.24491,0.35618,0.90378,0.87065,0.60258,0.44436,0.42884,0.50679,0.13952,0.41152,0.70113,0.23409,0.49667,0.34103,0.38241,0.26288,0.55515,0.90442,0.97851,0.41388,0.53890,0.32067],
        [0.40119,0.14152,0.93616,0.85221,0.47784,0.18590,0.61248,0.40275,0.00124,0.41462,0.30378,0.54611,0.75761,0.90713,0.23814,0.96548,0.83017,0.30166,0.12374,0.25842,0.81790,0.94146,0.43228,0.37306,0.91068,0.64487,0.98334,0.08815,0.69634,0.46613,0.19633,0.31327,0.08160,0.81890,0.81980,0.25240,0.90143,0.04806,0.45516,0.41416,0.06976,0.57012,0.06633,0.03789,0.49780,0.99122,0.36328,0.91714,0.08384,0.69439,0.12476,0.91812,0.22620,0.05223,0.11546,0.34614,0.06730,0.12909,0.72327,0.54222,0.71034,0.17659,0.75604,0.94035,0.97992,0.77916,0.98081,0.73778,0.39747,0.58635,0.00820,0.59875,0.73653,0.89173,0.58338,0.02559,0.92504,0.71298,0.12940,0.55400,0.93973,0.87985,0.18561,0.58553,0.54296,0.00061,0.05117,0.49895,0.76193,0.97400,0.39754,0.83883,0.48634,0.61868,0.83030,0.29901,0.37150,0.38559,0.06780,0.93134],
        [0.59174,0.70338,0.92862,0.27287,0.25511,0.60566,0.67317,0.19152,0.53877,0.56661,0.20019,0.61994,0.75737,0.56041,0.09958,0.00705,0.23901,0.58898,0.78825,0.04620,0.98161,0.86576,0.75971,0.77683,0.90973,0.74612,0.25668,0.06810,0.70917,0.14695,0.36292,0.49707,0.51486,0.35879,0.10425,0.56189,0.01452,0.90143,0.26502,0.53358,0.03388,0.86689,0.74560,0.62480,0.01581,0.73608,0.90826,0.51048,0.23917,0.93464,0.94692,0.79369,0.79460,0.43134,0.02530,0.26639,0.11946,0.75215,0.18972,0.69141,0.13939,0.91266,0.73478,0.63841,0.00459,0.13922,0.84498,0.44969,0.98962,0.00854,0.60320,0.66104,0.39297,0.08696,0.97606,0.43085,0.51483,0.31540,0.83131,0.95357,0.94886,0.35331,0.10789,0.89276,0.30508,0.73539,0.73190,0.00538,0.91620,0.94252,0.52249,0.66261,0.04497,0.19521,0.47318,0.43012,0.61708,0.51521,0.24202,0.43185]
    ]
    constraint = []

    for i in range(numPop):
        variable = variables[i]
        # variable = samRandom(n=numVar)
        # variable = samBeta(a=0.1, b=0.1, size=numVar)

        # variable = samUniform(low=0.0, high=1.0, size=numVar).tolist()
        print '\t[' + ','.join(map("{:.5f}".format, variable)) + '],'
        Individuals.append(Individual(estimator=estimator, variable=variable, constraint=constraint, weights=weights))
    return Individuals


# def moeaLoop(surrogate, population):
def moeaLoop():
    """moeaLoop"""

    _INF = 1e-14
    # _Ngen = 100
    # _Ndim = 10
    # _Npop = 4 * 5
    _Ngen = 10
    _Ndim = 100
    _Npop = 4 * 10

    _Nobj = 2
    _Ncon = 0
    fileName = './files/moea.json'
    CXPB = 0.9

    _SD = [] # Standard Deviation
    _SSD = [] # Scaled averaged Standard Deviation
    _Nret = 100 # Size of retraining pool

    # estimator = benchmarks.zdt1
    # estimator = benchmarks.zdt2
    estimator = benchmarks.zdt3
    # estimator = benchmarks.zdt4
    # estimator = benchmarks.zdt6
    surrogate = ANNSurrogate(algorithm='l-bfgs', alpha=1e-5, hidden_layer_sizes=(8), random_state=1)

    weights = (-1.0, -1.0)
    # weights = (1.0, 1.0)

    population = Population(numPop=_Npop, numVar=_Ndim, estimator=estimator, weights=weights)
    ioResultFile = jsonMOEA(fileName=fileName, numVar=_Ndim, numPop=_Npop, numCon=_Ncon, numObj=_Nobj, numGen=_Ngen)
    ioResultFile.writeHeader()

    print 'ANNSurrogate'
    X_scaler = StandardScaler()
    Xold_ind = np.zeros([_Npop, _Ndim])
    Yold_obj = np.zeros([_Npop, _Nobj])
    Xnew_ind = []
    Xnew_ind.append(samBeta(a=0.1, b=0.1, size=_Ndim))
    for ipop in range(_Npop):
        Xold_ind[ipop] = [deepcopy(X) for X in population[ipop].variable]
        Yold_obj[ipop] = [deepcopy(Y) for Y in population[ipop].fitness.values]
        # print '\t' + str(ipop) \
        #       + '\tXold_ind: [' + '\t'.join(map("{:.5f}".format, Xold_ind[ipop])) + ']' \
        #       + '\tMean_X: ' + str(np.mean(Xold_ind[ipop])) \
        #       + '\tStd_X: ' + str(np.std(Xold_ind[ipop])) \
        #       + '\tYold_obj: [' + '\t'.join(map("{:.5f}".format, Yold_obj[ipop])) + ']'
    X_scaler.fit(Xold_ind)
    surrogate.fit(X_scaler.transform(Xold_ind), Yold_obj)
    Ynew_obj = surrogate.predict(X_scaler.transform(Xnew_ind))

    print 'End\t\tXnew_ind: [' + '\t'.join(map("{:.5f}".format, Xnew_ind[0])) + ']' \
          + '\tMean_X: ' + str(np.mean(Xnew_ind[0])) \
          + '\tStd_X: ' + str(np.std(Xnew_ind[0])) \
          + '\tYnew_obj: [' + '\t'.join(map("{:.5f}".format, Ynew_obj[0])) + ']\n'

    igen = 0
    population = selNSGA2(population, _Npop)
    ioResultFile.writePareto(individuals=population, igen=igen)
    # print str(igen) + '\tGen:'
    # for ipop in population:
    #     print '\tpopulation.sel.a'\
    #           + '\tvar1: [' + ', '.join(map("{:.5f}".format, ipop.variable)) + ']'
    # print

    for igen in range(1, _Ngen):
        print 'Gen\t:' + str(igen)

        # print '\n' + str(igen) + '\tGen:'
        # for ipop in population:
        #     print '\tpopulation.sel.b'\
        #           + '\tvar: [' + ', '.join(map("{:.5f}".format, ipop.variable)) + ']'\
        #           + '\tobj: [' + ', '.join(map("{:.5f}".format, ipop.fitness.values)) + ']'\
        #           + '\tcrw: [' + str(ipop.fitness.crowding_dist) + ']'

        offspring = selTournamentDCD(population, _Npop)
        # for ipop in offspring:
        #     print '\toffspring.sel.a'\
        #           + '\tvar: [' + ', '.join(map("{:.5f}".format, ipop.variable)) + ']'\
        #           + '\tobj: [' + ', '.join(map("{:.5f}".format, ipop.fitness.values)) + ']'\
        #           + '\tcrw: [' + str(ipop.fitness.crowding_dist) + ']'
        # print

        offspring = [deepcopy(ind) for ind in offspring]
        # TODO 20161212 pass memeory address instead of values
        # for ipop in offspring:
        #     print '\toffspring.sel.a'\
        #           + '\tvar: [' + ', '.join(map("{:.5f}".format, ipop.variable)) + ']'\
        #           + '\tobj: [' + ', '.join(map("{:.5f}".format, ipop.fitness.values)) + ']'\
        #           + '\tcrw: [' + str(ipop.fitness.crowding_dist) + ']'
        # print

        for ind1, ind2 in zip(offspring[::2], offspring[1::2]):
            # TODO 20170109 rate
            if random.random() <= CXPB:
                # print '\toffspring.cx.b'\
                #       + '\tvar1: [' + ', '.join(map("{:.5f}".format, ind1.variable)) + ']'\
                #       + '\tvar2: [' + ', '.join(map("{:.5f}".format, ind2.variable)) + ']'
                ind1.variable, ind2.variable = cxSimulatedBinaryBounded(ind1.variable, ind2.variable)
                # print '\toffspring.cx.a'\
                #       + '\tvar1: [' + ', '.join(map("{:.5f}".format, ind1.variable)) + ']'\
                #       + '\tvar2: [' + ', '.join(map("{:.5f}".format, ind2.variable)) + ']'

            # print '\toffspring.mut.b'\
            #       + '\tvar1: [' + ', '.join(map("{:.5f}".format, ind1.variable)) + ']'\
            #       + '\tvar2: [' + ', '.join(map("{:.5f}".format, ind2.variable)) + ']'
            ind1.variable = mutPolynomialBounded(ind1.variable)
            ind2.variable = mutPolynomialBounded(ind2.variable)
            # print '\toffspring.mut.a'\
            #       + '\tvar1: [' + ', '.join(map("{:.5f}".format, ind1.variable)) + ']'\
            #       + '\tvar2: [' + ', '.join(map("{:.5f}".format, ind2.variable)) + ']'
            # print

            del ind1.fitness.values, ind2.fitness.values

        # Evaluate the individuals with an invalid fitness
        invalid_ind = [ind for ind in offspring if not ind.fitness.valid]
        ipop = 0
        for ind in invalid_ind:
            # ind.fitness.values = estimator(ind.variable)

            # TODO 20170105 implement fitness sampling rate & retraining ANN from anga [py:function:: estimator()]
            # find in varaible pool (cache)?
            #     <Yes> retreive fitness from cache
            #         Ynew_obj = ind.fitness.values
            #         ind.fitness.values = Ynew_obj[0]
            #     <No > model [estimator()]?
            #         <Yes> model [estimator()]
            #           ind.fitness.values = estimator(ind.variable)
            #           <> update cache and training set of ANN
            #           # Xold_ind[ipop] = [deepcopy(X) for X in ind.variable]
            #           # Yold_obj = ind.fitness.values
            #         <No > predict by ANN [ANN.predict()]
            #           Ynew_obj = surrogate.predict(X_scaler.transform(Xnew_ind))
            #           ind.fitness.values = Ynew_obj[0]
            #
            # retraining [ANN.fit()]?
            #     <Yes> surrogate.fit(X_scaler.transform(Xold_ind), Yold_obj)
            #     <No > ind.fitness.values = ind.fitness.values

            Xold_ind[ipop] = [deepcopy(X) for X in ind.variable]
            Yold_obj[ipop] = [deepcopy(Y) for Y in estimator(ind.variable)]
            # TODO 20170105 update retraining pool
            surrogate.fit(X_scaler.transform(Xold_ind), Yold_obj)
            Xnew_ind[0] = ind.variable
            # TODO 20170105 predict by ANN
            Ynew_obj = surrogate.predict(X_scaler.transform(Xnew_ind))
            # print '\t' + str(ipop) \
            #       + '\tXnew_ind: [' + '\t'.join(map("{:.5f}".format, Xnew_ind[0])) + ']' \
            #       + '\tMean_X: ' + str(np.mean(Xnew_ind[0])) \
            #       + '\tStd_X: ' + str(np.std(Xnew_ind[0])) \
            #       + '\tYnew_obj: [' + '\t'.join(map("{:.5f}".format, Ynew_obj[0])) + ']'
            ind.fitness.values = Ynew_obj[0]

            # Xold_ind[ipop] = [deepcopy(X) for X in ind.variable]
            # Yold_obj[ipop] = [deepcopy(Y) for Y in estimator(ind.variable)]
            # surrogate.fit(X_scaler.transform(Xold_ind), Yold_obj)
            # Xnew_ind = np.array(ind.variable)
            # Ynew_obj = surrogate.predict(X_scaler.transform(Xnew_ind))
            # print '\t' + str(ipop) \
            #       + '\tXnew_ind: [' + '\t'.join(map("{:.5f}".format, Xnew_ind[0])) + ']' \
            #       + '\tMean_X: ' + str(np.mean(Xnew_ind[0])) \
            #       + '\tStd_X: ' + str(np.std(Xnew_ind[0])) \
            #       + '\tYnew_obj: [' + '\t'.join(map("{:.5f}".format, Ynew_obj[0])) + ']'
            # ind.fitness.values = Ynew_obj[0]

            ipop += 1


        # print 'Select the next generation population\nAfter cx mut'
        # for ipop in population:
        #     print '\tpopulation.sel.b'\
        #           + '\tvar: [' + ', '.join(map("{:.5f}".format, ipop.variable)) + ']'\
        #           + '\tobj: [' + ', '.join(map("{:.5f}".format, ipop.fitness.values)) + ']'\
        #           + '\tcrw: [' + str(ipop.fitness.crowding_dist) + ']'
        # for ipop in offspring:
        #     print '\toffspring.sel.b'\
        #           + '\tvar: [' + ', '.join(map("{:.5f}".format, ipop.variable)) + ']'\
        #           + '\tobj: [' + ', '.join(map("{:.5f}".format, ipop.fitness.values)) + ']'\
        #           + '\tcrw: [' + str(ipop.fitness.crowding_dist) + ']'
        # print
        population = selNSGA2(population + offspring, _Npop)
        ioResultFile.writePareto(individuals=population, igen=igen)
        # for ipop in range(_Npop):
        #     print '\tpopulation.sel.a' \
        #           + '\tXold_ind: [' + ', '.join(map("{:.5f}".format, population[ipop].variable)) + ']'\
        #           + '\tYold_obj: [' + ', '.join(map("{:.5f}".format, population[ipop].fitness.values)) + ']'\
        #           + '\tcrw: [' + str(population[ipop].fitness.crowding_dist) + ']'

        # for ipop in range(_Npop):
        #     population[ipop].objective = population[ipop].estimator(population[ipop].variable)
        #
        #     Xold_ind.append(population[ipop].variable)
        #     Yold_obj.append(population[ipop].objective)
        #
        #     print '\t' + str(ipop) \
        #           + '\tXold_ind: [' + ', '.join(map("{:.5f}".format, population[ipop].variable)) + ']'\
        #           + '\tMean_X: ' + str(np.mean(population[ipop].variable))\
        #           + '\tStd_X: ' + str(np.std(population[ipop].variable))\
        #           + '\tYold_obj: [' + ', '.join(map("{:.5f}".format, population[ipop].objective)) + ']'
        #
        # surrogate.fit(Xold_ind, Yold_obj)
        # Ynew_obj = surrogate.predict(Xnew_ind)
        # print 'ANNSurrogate.Xnew_ind:\n\t[' + '\t'.join(map(str, Xnew_ind)) + ']'
        # print 'ANNSurrogate.Ynew_obj:\n\t[' + '\t'.join(map(str, Ynew_obj)) + ']'

    ioResultFile.writeEnd()

    ioResultFile.plot_json()




if __name__ == "__main__":
    moeaLoop()


    # surrogate = ANNSurrogate(algorithm='l-bfgs', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=1)
    # surrogate.fit(Xold_ind, Yold_obj)
    # y_pred = surrogate.predict(Xnew_ind)
    # # print surrogate.regressor
    # print 'ANNSurrogate.y_pred: ['+', '.join(map(str,y_pred))+']'


    # from sklearn.neural_network import MLPRegressor
    # regressor = MLPRegressor(algorithm='l-bfgs', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=1)
    # regressor.fit(Xold_ind, Yold_obj)
    # y_pred = regressor.predict(Xnew_ind)
    # # print regressor
    # print 'MLPRegressor.y_pred: ['+', '.join(map(str,y_pred))+']'
